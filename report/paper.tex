% CS 410 Summer 2014 LaTeX template, based off of
% http://www.acm.org/sigs/publications/proceedings-templates

\documentclass{acm} % this finds the file "acm.bst"
\usepackage{easyfig}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage{CJK,CJKspace,CJKpunct}
\usepackage{cite,graphicx}
% \usepackage[T1]{fontenc}

% this lets you include clickable URLs with \url{}
\usepackage[hidelinks]{hyperref}

\easyfigdefault{placement={!htb}, max width=19pc, max height=.5\textheight, keepaspectratio}

\pdfmapline{=unisong@Unicode@ <ipam.ttf}
\begin{document}\sloppy % sloppy necessary here
\begin{CJK}{UTF8}{zhsong}
\title{JLyrics}

\numberofauthors{2} % make sure you set this number
\author{%
    \alignauthor Larry Resnik \\
    \affaddr{Department of Computer Engineering}\\
    \affaddr{University of Illinois at Urbana-Champaign}\\
    \email{lsresni2@illinois.edu}
    \alignauthor Zach Bian\\
    \affaddr{Department of Computer Engineering}\\
    \affaddr{University of Illinois at Urbana-Champaign}\\
    \email{zbian2@illinois.edu}
}
\maketitle

\begin{abstract}
JLyrics is a search engine that finds the Japanese song that some lyrics came from.
\end{abstract}

\keywords{japanese; music; search; anime; vocaloid}

\section{Introduction}

JLyrics is the name we gave our final project for CS 410. Our project was chosen to solve certain problems we had noticed in terms of information retrieval. For starters, looking up what song had a particular set of lyrics would not necessarily bring you to the same website storing those lyrics as another website also storing song lyrics. The information we want is sparsely distributed and uploaded. For example, if you do not know if the song originated from an anime or from a video game, you would not know whether to search an anime songs website or a video game songs website for the song. We wanted to combine the information sets of both or multiple websites so that you could be sure to run into the song you want to find.

\section{Motivation}

As for why someone would want to look up the song that some lyrics came from, there are a couple answers to that question.

First of all, a song one may come across by chance may have little information attached to it. Perhaps the song came from a medley or from a YouTube re-upload without a detailed title or tags. In that case, the listener has no background to go on to find the song aside from the lyrics they have heard. Moreover, human memory and recognition is finite. If we were to passively hear a song we like, we could only catch portions of the song's lyrics. Ideally, just those lyrics alone should be enough to help someone find that song.

Secondly, finding information on the song itself can assist the listener in learning foreign languages. Repetition is the best methods for learning. Listening to enjoyable music and recognizing new words each time the song is played is a fun way to strengthen one's language capabilities through repetition. Many music players such as foobar2000 and Rocket Player will display the lyrics of a song that is being played so long as those lyrics have been embedded into the music file as a tag. An example screenshot of Rocket Player showing a song's lyrics is in Figure~\ref{fig:res/rocket-player.png} This feature can be used in tandem with the learner's interest in following along with the song to promote learning the language. If one is clever, they could put the same lyrics into the karaoke variant of the song and practice singing to improve their reading and speaking abilities. Regardless of the reason, they user would need to find the full lyrics from the song. We want to help with this cause.

\Figure[caption={Rocket Player Displaying Song Lyrics}, max height=.3\textheight]{res/rocket-player.png}

Finally, if someone finds a song they like, there is a good chance that the songs made by that song's producer would also be likable. 

\section{Goals}

We want to build a search engine that connects the world of Japanese songs to non-Japanese speakers. To that extent, we had a list of goals to work on to drive the development of the project.

Our target audience are people who are interested in songs sung in anime, in doujin music, or by Vocaloid artists. These interests tend to be intermixed because of websites that host all of these things at once such as Niconico Douga. Because the non-Japanese speaking fans of these materials are numerous, this search engine could be useful for them. The intermingling of sources creates a rich and diverse community, but the people who are introduced to the Japanese song scene would not necessarily understand the difference between a Vocaloid song and a Touhou doujin song for instance. Using our search engine should provide no boundaries to that inadequacy.

%Aside from that, the language barrier that a non-Japanese speaker has to deal with when looking up Japanese is having to figure out if their search results bore fruit or not. The first result may be a perfect match, but if the searcher can't read Japanese in the first place, they would have no idea just how accurate their search was. It would be nice if we could present to the user their search results in a format they could understand and work with such as translated text or transliterated text (written in romaji).

In order to merge these data points of interest into one search engine, we need to scour the Internet to get diverse sources of data. Although we want a lot of data, we also want to minimize overlap in the data we scour. Crawling the Internet is a costly operation in terms of time because we must wait between requests to get web pages. We decided to target three sites in particular in order to balance the amount of songs we could fetch and the estimated amount of overlap we may encounter.

\begin{itemize}                                                                                                                                                                                             
\item ピアプロ (Piapro): \url{http://piapro.jp/}
\item Anime Lyrics dot Com: \url{http://www.animelyrics.com/}
\item 東方同人ＣＤの歌詞 (Touhou Doujin CD no Kashi): \url{http://www31.atwiki.jp/touhoukashi/}
\end{itemize}

Piapro is a famous site for Vocaloid producers. They upload their songs, lyrics, karaoke versions of their songs, and other things to Piapro. It is a valuable site because a huge portion of songs uploaded to Niconico Douga in video format are also linked to Piapro. This typically is done to show fans to the karaoke versions of songs so that the fans could upload videos of themselves singing to the song. In other words, Piapro covers a huge fanbase for Vocaloid and 歌ってみた (utattemita / Tried to sing) fans.

Anime Lyrics dot Com would be our most valuable source of song data because, as the name implies, it is a massive database of anime songs and their lyrics. This site stores lyrics for the purpose of non-Japanese speakers, so it stores them transliterated first and foremost. The song may or may not have kanji and/or English lyrics available as well. Due to the uncertainty of which songs have lyrics or not and what formats are available, Anime Lyrics dot Com appears to be the site that will take the most development time to crawl.

Touhou Doujin CD no Kashi is a Wikipedia site dedicated to storing the lyrics of Touhou doujin songs. The Touhou fanbase has made a huge array of music and the fans are also numerous these days. On the same token, a listener may not recognize a song as a remix from a Touhou game, so this search engine would not be troubled by the discrepancy. Because Touhou Doujin CD no Kashi is a Wikipedia site, it would take a considerable amount of development time to scour and parse.

\section{Crawling}

Before we could make a search engine, we would need to have material to search through. The Internet already has a great amount of data on it, but we need to extract that information. In order to do that, we used programming languages that were able to retrieve web pages so that we could save those pages on our hard drives. Later, we would program some way to parse those pages and get the data we wanted. The final product would be a standardized format for documents (a.k.a. docs) that the indexer would interpret.

\subsection{Anime Lyrics dot Com}

\Figure[caption={Site Logo}]{res/logo_animelyrics.png}

To begin with, deciding how best to crawl Anime Lyrics dot Com took a considerable. The site's multi-layered structure will be explained shortly. The home page lists all of the genres of music stored on Anime Lyrics dot Com. Despite what their name may imply, they also store lyrics for dance, dance CD, doujin, video game, and J-Pop music. Each genre has an index.htm file listing all of the albums belonging to that category. Each album name points to an index.htm in a deeper directory. The album page at this area lists all of the songs in that album. The songs are in the same directory as the album, but they do not have the name index.htm, so they stand out in the address bar. The name index.htm is a default name that the web browser will try to access, so the URL usually omits this. By consequence, we only see a ``.htm'' URL when looking at a song. This was how Anime Lyrics dot Com was designed as seen from a surfer on their web site.

In summary, the general structure of Anime Lyrics dot Com can be graphically seen in Figure~\ref{fig:res/animelyrics_format.png}. All folders shown have an index.htm in them, but the album folders all hold song web pages as well.

\Figure[caption={Anime Lyrics dot Com Structure}, max width=5pc]{res/animelyrics_format.png}

The crawler would get all index.htm pages and also the pages not named index.htm which were actual songs. When the pages are parsed to generate the indexable docs, the folder structure is retained, but there will be no more index.htm pages left over. The only files left over would be plain text documents parsed from the songs which would later be indexed by the search engine.

As for implementation, all Anime Lyrics dot Com crawling and parsing was done in Python 2 using the Beautiful Soup (2) library and a few regular expressions. All crawling was done with a one-to-two second randomized delay so as to not hammer their server. Two scripts were ran at once simultaneously on the same computer to get more songs at once during the final phase of the crawling.

At the topmost level of the site, the genre pages were grabbed by hand and the full list of albums were retrieved from that. The album pages were stored locally, but it was quickly discovered that even the plain text produced by Anime Lyrics dot Com created an extremely large amount of data that was stored. About 60 MB of album pages were downloaded. Most of this was boiler plate code such as the navigation bar, embedded scripts, and formatting to make the web page display nicely. With that, we had delved into the two highest layers of the site: the home page's genres and the genre's albums.

Before beginning the crawling of the song pages, a method of getting rid of the unnecessary data was worked on. Without making too many assumptions about the format of the nearly 40,000 song pages Anime Lyrics dot Com had available, it was decided that the chunk of code above and below the song lists of each album page would be cropped out before being saved to the hard drive. As an extra precaution, Beautiful Soup was used to re-create valid HTML before saving the crawled album page. This was to ensure that later parsing of the song page would hopefully be consistent. This was how disk space was saved during the crawling process.

During the development process, bugs were discovered when attempting to run the code on different systems. A total of four different systems had seen work with the code base being worked on. Python 2 with Beautiful Soup 2 had ran the script on Windows 7, Windows 8, Xubuntu (Linux), and Scientific Linux (Linux). In doing so, non-cross-platform bugs surfaced in the code. Of particular interest was that the crawler would outright crash on Windows. The problem was that Beautiful Soup was capable of using a slew of different HTML parsers and it would select a default for you. The default was usually a bad one that shipped with Python, so on the Windows operating systems, the lxml library was installed as well. With all three systems using lxml for parsing the web pages, the code became consistent in behavior and cross-platform. The Scientific Linux system happened to be the UIUC EWS machines. The University did not give users root privileges, so it was too much of a hassle to install lxml there. The code base was at least managed to get running on three of the four systems.

With all album pages crawled, the song links needed to be extracted. This was tricky because Anime Lyrics dot Com does not store its lyrics with Japanese text in the same page as where it stores the translated and/or transliterated text. For example, one of the songs from K-On! named ``NO, Thank You!'' was popular enough to have its original Japanese, a translation, and a transliteration uploaded. However, the transliteration and translation are found at this URL:

\url{http://www.animelyrics.com/anime/kons2/nothankyou.htm}

Whereas the Japanese text could be found at this URL:

\url{http://www.animelyrics.com/anime/kons2/nothankyou.jis}

All songs had a transliteration by default, but a check would be needed to done to see if Japanese text was available. The method devised was to look at the album page for a particular image next to the song name. This image was a picture of the Japanese word 漢字 (kanji meaning Chinese characters). Its tool tip text says ``Japanese Text available'' implying that a separate Japanese lyrics page was on the server. An example of this is shown in Figure~\ref{fig:res/kon_album.png}. This is an album list from K-On! as shown on Anime Lyrics dot Com. The song ``Our MAGIC'' does not have Japanese text, but the song ``NO, Thank You!'' does.

\Figure[caption={K-On! Album List}]{res/kon_album.png}

Beautiful Soup made it simple to extract all of the links needed. It was a one-line statement in Python to see if a Japanese page was available. As opposed to traversing the album's table storing the songs, each table entry storing a song was checked for the Japanese signifier using the following code:

\begin{lstlisting}[language=Python]
has_kanji = len(soup_song.parent('img',
	alt='Japanese Kanji available'))>0
\end{lstlisting}

With the ability to check for Japanese pages, Beautiful Soup was furthermore used to extract all anchor tags (HTML entities defined by <a></a>). Since song names did not have the address of index.htm, those links were removed. After removing a small set of unusual edge cases of anchors not linking to songs (such as anchors intended to work with javascript), all song links had been retrieved. The song links were put into a massive text document that simply separated each link by new lines. This resultant file was 1.35 MB large. With all song links extracted, it was time to get the songs.

After the crawler had finished retrieving all of the song pages, a total of 42,682 songs were stored locally. Even after trimming the junk content from these songs, 600 MB of data had been stored locally. All of this data would need to be parsed into documents. Due to time constraints, it was decided to only check the 12,654 Japanese lyrics crawled because the docs produced from them would definitely work correctly with the Piapro docs being crawled by the other team mate.

\subsection{Piapro}

\Figure[caption={Site Logo}]{res/logo_piapro.png}

Piapro was a fairly simple site to crawl due to the site's structure. The pages were all indexed by number, which could be accessed via changing the URL itself. The first page could be accessed at \url{http://piapro.jp/text/?categoryId=7\&page=1}. Changing the URL page number past the last page returns a page that says ``見つかりませんでした。'' or ``Couldn't find it.'' Each of these 4710 pages lists thirty songs, for a grand total of just over 141,000 songs. These links are stored under \url{http://piapro.jp/t/} on the site itself. Each song has a unique id of four characters, such as ``KlF-'', which goes after the previous url. These pages contain the title, author, and lyrics data we needed.

As for implementation, at first C++ was used to call ``curl'' (via <cstdlib>'s ``system()'' function) first on the list pages to get all the song id's into a file. Next, the program went through this list of song id's and called curl again on each of the URL's, pulling the required data from the downloaded HTML file and writing them to files.

Just for convenience, a progress checker that kept track of the number of songs was added. Furthermore, a time delay was used to prevent too many requests being sent at once. This was a small random integer of seconds.

Due to the sheer number of songs to crawl, it became necessary to distribute the processing over a number of computers. Another issue was that EWS lacks the required libraries to compile this code or even run the executable. As a result, a simplification needed to be made. This time, the crawler simply took the HTML files, so that they could all be processed on a machine with the required library files. In short, the original crawler was split into two parts. As a result, the following scheme was used: (Note: Ideally, the number of links would have been 141,303, but due to the site being updated, there were duplicate links.)

\begin{itemize}
 \item Links 1-69,999: Zach's laptop
 \item Links 70,000-79,999: Zach's laptop via ssh to EWS
 \item Links 80,000-99,999: EWS machine no. 1
 \item Links 100,000-141,357: EWS machine no. 2
 \item Total curl commands required: 146,067
\end{itemize}

The actual time spent crawling across the four machines was around 3 days. Otherwise, on a single machine, a reasonable estimate would have been around 5-6 days. Had we performed this estimation sooner, we could have distributed the download and saved another day or so.

\section{Doc Creation}

The docs are the plain text files that our indexer would parse into something that can be searched. For that to happen, we would need to define a consistent format that all of our songs could be described in. We would need to be able to produce this format regardless of which web site the data came from. Our format was chosen to be the following.

\begin{itemize}
\item URL of the song on line 1.
\item Song title on line 2.
\item Artist/Producer of the song on line 3.
\item Language of the lyrics on line 4.
\item Song lyrics on line 5 and on.
\end{itemize}

The URL would give searchers a traceback to the source of where we got the song lyrics from. That is how the user can learn more about the song. The song's title and artist are a shortcut in case the user only cares about looking those up such as for tagging music or Googling outside of our search engine and the source sites. The language would differentiate between Japanese, transliterated, and translated text. It may play a role in the searcher if we had supported more than just Japanese text, but for now it is simply an extra byte of information that our search engine ignores. Finally, the lyrics are both what the search engine compares the query with and what the user uses to verify that a song s/he was looking for was the right one.

\subsection{Anime Lyrics dot Com}

Document creation in Anime Lyrics dot Com was a tough thing to do. The song title was easy to get because it was inside of a header (<h1>) tag, but its translated title had to be extracted using some string searching. The title was always transliterated, but if there was a translation available, it would appear on a new line in the source code preceded by a line break tag (<br>). The language of the documents were hard coded to represent ``Japanese'' because, due to deadlines, only the Japanese songs were parsed into docs. The URL to the song's source was regenerated based on the path to the song stored from the indexed files.

The hardest part of generating the docs by far were getting the lyrics. Anime Lyrics dot Com implemented a method by which you could hover your mouse over a kanji and see details about it. This was accomplished by surrounding absolutely every single kanji character in a song's lyrics by anchor tags. Beautiful Soup was used to transform every anchor tag found in the lyrics into just the text within the anchor tag. For example, the site may have the following coded into the HTML: <a class="supernote-hover-kanji0" name="kanji">渇</a>. Beautiful Soup would transform all of that into simply 渇. In doing so, we would be able to remove all junk formatting from the lyrics and be able to store it in plain text format for our docs.

In practice, this was not enough to produce good docs. Due to extra spacing put in between kanji in the source code of Anime Lyrics dot Com web pages, the text without formatting had a huge amount of extra white space left behind. This was a problem because every word in Japanese formed by kanji compounds such as 明日 would appear as 明 日. We could not expect a search engine or indexer to properly recognize that as a word that was a kanji compound. The naive solution would have been to remove all spacing between characters, but this solution backfired. When the song had mixed Japanese and English lyrics, the English lyrics would become squished. For example, the song Shigunaru from the anime 07-Ghost has one line of lyrics that goes ``潰れたくないのなら BEGIN TO MOVE ON SHINING''. Using the naive regex that removed spaces between any two characters, this would become ``潰れたくないのならBEGINTOMOVEONSHINING''. A more strict solution needed to be developed.

We needed to find a way to remove all white space between two \emph{Japanese} characters, but ignore white space between other characters. Python 2's regex library was not expressive enough to deal with Japanese characters, but there was a library in the works intended to replace Python's current regex API for a more comprehensive one. Specifically, the ``regex'' module could be downloaded from PyPi to replace Python's ``re'' module. This allowed referencing whole character sets such as all katakana characters. A string representing all Japanese characters for a regex was produced with the following Python code.

\begin{lstlisting}[language=Python]
RE_JAPANESE_GLYPH = '[\p{Hiragana}' +
    '\p{Katakana}\u4e00-\u9faf\W\u3005]'
\end{lstlisting}

Using this regex string to find a single Japanese character, we create our regex that looks for white space between two Japanese characters.

\begin{lstlisting}[language=Python]
spaced_jp_regex = re.compile(
    RE_JAPANESE_GLYPH + '\s+' +
    RE_JAPANESE_GLYPH, re.UNICODE)
\end{lstlisting}

This regex will match ``明 日'', but fail on the following three cases: ``日　O'', ``N 日'', and ``N O''. We need this because the regex will fail on ``BEGIN TO'' and therefore not turn the string into ``BEGINTO''. By using this regex continuously on the crawled web page until the regex no longer finds matches, all of the lyrics will appear exactly as they were intended to be. This produced the lyrics for the song. With the lyrics on hand, we could produce the indexable docs for Anime Lyrics dot Com.

\subsection{Piapro}

Turning the crawled pages from Piapro into indexable docs was a simple process. The artist was simple to retrieve, the lyrics were not particularly mangled with formatting, and the language byte we would store would unconditionally represent ``Japanese''.

In order to parse each of these HTML files, since the characters were in UTF-8, a setlocale() was required. Also, the glibmm library provides wrappers for processing UTF-8 strings with the Glib::ustring data type, which can be used much in the same way as the std::string data type. It also provides a regex library for UTF-8 regular expressions. As the pages were downloaded, each line was checked for matching regular expressions.

The C++ code made extensive use of lookaheads and lookbehinds to facilitate writing directly to the documents. These were determined by observation of the HTML pages.

\section{Indexing and Searching}

A search engine will have nothing to search for if the docs are not indexed. Everyone in CS 410 had the background required to build a search engine, but for the summer semester, there was not enough time to implement an indexer and a searcher. In order to alleviate this and allow us to understand search engines in general, the professor had us all do week-long homework problems using a program he developed. This program, Goose, was a Java application built around Lucene. Lucene is an API for implementing an indexer and a searcher. In order to save development time, we used Goose as the base to JLyrics.

Goose was built to work with English text, so it would not work with our data set right out of the box. The only change we had to make was to discover how to replace the self-made text analyzer in Goose with a JapaneseAnalyzer that comes with Lucene. This JapaneseAnalyzer was a library originally called Kuromoji. Kuromoji was donated to Lucene and became a part of Lucene's API \cite{kuromoji-donate}. Using JapaneseAnalyzer, we were able to create a working indexer and searcher.

\section{Results}

Due to time constraints imposed by other classes that both team members had to deal with, only the bare minimum working product with rudimentary testing in the end. The search engine works from command line because there was not enough time left over to try to convert our project into a website. Out of all of the websites we wanted to crawl, only Piapro and the Japanese text uploaded to Anime Lyrics dot Com were saved. So altogether, we crawled around 13,000+40,000+141,000+5000 = 199,000 pages.

Unfortunately, because the pre-processed HTML files are considerably larger than the post-processed ones, the EWS machines ran out of space. Due to this unforeseen occurrence, we didn't think to save space by clearing out unnecessary files, and only around 1GB's worth of HTML files could be downloaded. In reality, the number of documents indexed only ended up being 127,223 instead of an estimated 154,000.

After indexing all of these documents with Goose, we performed a search using the -{}-search option. As shown in Figure~\ref{fig:res/410_post_Piapro.png}, a search for the first line of the lyrics of ``Hare Hare Yukai'' yielded good results.

\Figure[caption={Successful Search Example}]{res/410_post_Piapro.png}

\subsection{Platform Dependent Results}

Unfortunately, the project would not work under Windows. Windows support was a feature that we assumed that we would get for free just because we used the cross-platform programming language Java to do our indexing and searching. Naively attempting to run the searcher in Windows will most likely evoke mojibake. 文字化け (mojibake meaning ``changed characters'') is the result of an encoding scheme being unable to properly represent a bit pattern.

In order to understand the problem with mojibake, a discussion of character encodings will need to be presented. A single character such as the letter ``a'' must be represented in a format that a computer can understand. At its absolute lowest level, a computer can only store data in the form of ones and zeros. The number system that contains only these two numbers is called binary. However, the string of bits we choose to encode a particular character in is completely arbitrary. To that extent, the ASCII (American Standard Code for Information Interchange) format was invented so that everyone programming on computers would follow the same format for encoding a set of characters. For example, ``a'' is represented in ASCII format as ``0110 0001''. ASCII was invented in the very young days of computing, so there was no intention of supporting alphabets and character sets beyond what Latin characters could represent (for example, English).

Meanwhile, Japan was making headway in computing but was in dire need of a standard way to represent Japanese characters. The format called Shift-JIS (Shift Japanese Industrial Standards) was invented. It was created for representing Japanese characters such as kanji, hiragana, and katakana in standardized bit patterns, but it was also capable of representing Latin characters (called romaji / ローマ字 / roman characters in Japanese). This format solved Japan's rendering problems, but it was not a very cooperative encoding format when interpreted by programs and operating systems not expecting the Shift-JIS format.

As time went on, a new standard of expressing characters as strings of bytes appeared called Unicode. It proposed blueprints of how one should represent characters form any and all alphabets in a set length of bits. Unicode is an abstract concept \cite{unicode-abstract}. The most popular encoding of Unicode to this day is UTF-8 (Unicode Transformation Format 8). Many Japanese developers are content with Shift-JIS even though the Shift-JIS encoding can't handle non-Japanese and non-Latin characters, so there is a struggle involved with dealing with Japanese text such that one must check if the text was encoded in Shift-JIS, UTF-8, or something else.

To illustrate just how drastically important keeping our encoding scheme straight is, a demonstration of various encodings of the same Japanese word is shown using Python 3.

\begin{verbatim}
>>> "文字"
'文字'
>>> "文字".encode("utf-8")
b'\xe6\x96\x87\xe5\xad\x97'
>>> "文字".encode("shift-jis")
b'\x95\xb6\x8e\x9a'
>>> "文字".encode("cp932")
b'\x95\xb6\x8e\x9a'
\end{verbatim}

This word, 文字 (moji meaning character(s)), shows its binary representations as it would be if it were encoded in UTF-8, Shift-JIS, and Windows Code Page 932. Notice how Windows CP-932 is the same as Shift-JIS.

The pages we took from Piapro were stored in UTF-8, but we had a good number of pages from Anime Lyrics dot Com that were stored in Shift-JIS. Parsing these web pages and producing indexable documents from them required careful attention of the encodings each web page was chosen to be encoded in.

Despite all efforts to keep our encodings straight, nothing could get Windows' command prompt, cmd, to behave as we would have wanted it to. As shown in Figure~\ref{fig:res/cmd_search_fail.png}, trying to look for the words 聞こえる and 愛 fail to produce results. We wonder if it is because the words we pass to the Java program through cmd are getting re-encoded and thereby messing up our results.

\Figure[caption={cmd.exe on Windows Failure}]{res/cmd_search_fail.png}

While testing our project, we had access to Linux computers through SSH thanks to the University of Illinois at Urbana Champaign's Engineering WorkStations (UIUC's EWS). By using the Windows program called PuTTY to interact with the Java-based searching program, we got to see perfect results as shown in Figure~\ref{fig:res/putty_search_success.png}. It raises the question of where the encoding transformations could be going wrong since PuTTY appears to be a terminal application just like cmd.

\Figure[caption={PuTTY on Windows Success}]{res/putty_search_success.png}

If we could output the search results to a web page, the encodings would be the problem of the web browser instead of the terminal. Because popular web browsers are intended to be used by people worldwide, they tend to support many text encoding formats, so there is may be better results by having a web browser interact with the Java program. We did not have the time to work on the web browser implementation though.

\section{Experiments}

There were a few things we tried out to see how effective our search engine was. Even though the engine works just fine and has a considerable amount of data in its back end, it would still be a bad search engine if it behaved improperly.

%The first thing we checked was if distance between words mattered at all. For example, if we were to search for 愛が欲しいと叫んだ声なら ("if you were to cry out that you wanted love"), then the first hit of the search engine should ideally be the song that this came from both because the words are close together and the unique words all come from the same song.

\subsection{Against Google Custom Search}

A milestone we wanted to achieve with this project was to beat Google Custom Search using our vertical search engine. Anime Lyrics dot Com has a built-in search engine on their web page, but it is simply a Google Custom Search. It would be identical to searching on Google for ``site:animelyrics.com'' followed by the song lyrics. Our test was performed by choosing a specific song and some lyrics from it. Afterwards, we would change one of the words to see if the search engine still picks up the original song.

Our chosen song was シリウス (Sirius) from the KILL la KILL sound track. The lyrics would be the very first line: 誰かが勝手に決めた (someone decided on their own that...). The bastardized line would be this: 誰かが勝手に急に (someone had immediately, without permission did...).

Google Custom search performs perfectly when looking up the the correct line. They rank it number one out of three. This is shown in Figure~\ref{fig:res/google_custom_correct.png}. Our search engine however ranks Sirius at rank 10 out of 25.

\Figure[caption={Google Search for Sirius: Success}]{res/google_custom_correct.png}

When using the invalid text, Google Custom search fails to find the test song at all. The results are shown in Figure~\ref{fig:res/google_custom_wrong.png}. Our search engine also fails to find the song.

\Figure[caption={Google Search for Sirius: Failure}]{res/google_custom_wrong.png}

The lesson learned here is that perhaps we were expecting too much out of search engines. Our search engine could not compete with Google's, but neither one functioned well on improperly remembered lyrics. The other lesson is that we probably can't compete with the professionals with a naive outlook. We may have lost the match, but it gives us a nice high bar to beat should either of us attempt to improve the search engine.

%\subsection{Kanji Count Variations}

%While testing our search engine, we recognized a pattern. It seemed that the search engine worked best when there were more kanji in the search query. We decided to test this hypothesis.

\section{Related Work}

In terms of related work, Anime Lyrics dot Com did precisely what we have attempted to do. Their site conglomerates Japanese music lyrics from multiple venues so that users can search for lyrics in Japanese, romaji, or English. The amount of data they have is proportional to the amount of manpower put into the site. We wanted to have a more automated system so that people could stay up-to-date with popular music scenes such as Vocaloid music uploaded to Piapro or Touhou remix music detailed on the Touhou Doujin CD no Kashi Wikipedia site. To that extent, we could arguably state that we are following in the footsteps of Anime Lyrics dot Com while simultaneously using their work.

%What similar things are already out there? You can cite things like this~\cite{manning-ir-book} and this~\cite{zhai-smoothing}. I got the second reference bibtex from \url{http://dl.acm.org/citation.cfm?id=984322}. I just simplified the bibtex reference names to ``manning-ir-book'' and ``zhai-smoothing''. Also, did you notice that URL and citations were clickable?

\section{Discussion}

Are there any interesting questions or findings that should be discussed?

\section{Future Plans}

Due to time constraints, we were unfortunately unable to implement some of the items that we wanted to get to.

Firstly, we think that the creation of a webpage would have been a nice addition to the project. To give a more concrete idea, certain websites have a built-in custom search bar and display search results in the browser. This would be a big advantage over the command line searcher because command line applications requires special knowledge of the command line and terminal applications can't handle Japanese characters if the font does not support the proper size of the characters (backspacing misaligns the cursor).

Another big issue we ran into was when we tried to run the application on Windows' command line. Windows has its own method of text-encoding. The first major consequence is that results do not render properly. For some reason, only some of the lyrics' kanji would show up correctly, but not all of them. The second major consequence is that the search does not perform properly. Searching for any Japanese text yields no results at all, and putting the same English query into Linux and Windows yield different results.

As for additional Japanese support, we could provide disambiguation of synonyms in the query. By synonym here, we mean that a word could have a kanji version(s), a kana version, and a romaji version, either in full- or half-width. For example, there are many ways to write the verb ``to take'' such as とる, 取る, and 撮る. These are all pronounced ``toru,'' but each has different nuances. We might not know which of these is in the song entry, so this would make searching easier on the user.

Speaking of synonyms and the user, the user may lack adequate knowledge of the Japanese language to read the search results. It would also be convenient to give the user the option of getting results in Japanese or its Romanized form.

We also wanted to crawl 東方同人ＣＤの歌詞 (Touhou Doujin CD no Kashi): \url{http://www31.atwiki.jp/touhoukashi/}. This is a giant database of songs that neither Anime Lyrics dot Com nor Piapro have. The Touhou fanbase has made many songs, so it would have given us a lot more data to work with. The creation of another crawler would have been a lot more work on its own, though.

Lastly, the addition of Anime Lyrics dot Com's transliterated lyrics and translated lyrics would have made a good addition to our database. Currently, our data only consists of Japanese songs, which requires some degree of knowledge of the language to use. With this, English speakers could search a English translation of the song as well.

\section{Conclusion}

Our project successfully indexes and searches a large database of Japanese songs. While it is regrettable that we could not also get to the Touhou Doujin CD site, we were able to get most of Piapro and all of Anime Lyrics dot Com's Japanese lyrics into the index. That is to say, this project is scalable to larger data sets. We have seen that for each of these sites, crawlers had to be adapted to retrieve the required lyrics from them. After crawling, it was necessary to generate readable documents to be indexed via parsing. After finally getting all 127,000 of the documents, we ran the Goose indexer on the data set with a Japanese Analyzer. Lastly, we performed search tests. While there were limitations on its efficacy, we could show that the searching worked reasonably well. We also discussed possibilities of expansion, whether with larger data sets, capability, or user-friendliness.

\section*{Acknowledgments}

Sean Massung: Professor of CS 410: Information Retrieval Systems. Also the developer of Goose, the combination indexer and searcher built on top of Lucene. JLyrics is essentially Goose with minor edits to incorporate support for Japanese text.

\appendix

\section{Division of Labor}

\begin{itemize}
\item Crawling and Parsing of Anime Lyrics dot Com: Larry
\item Crawling and Parsing of Piapro: Zach
\item Initial Outline: Larry, Zach
\item Midterm Report: Larry, Zach
\item Lucene Setup: Larry
\item Final Report: Larry, Zach
\end{itemize}

\bibliographystyle{plain}
\bibliography{bib} % "bib" is the name of the .bib file

\end{CJK}
\end{document}
