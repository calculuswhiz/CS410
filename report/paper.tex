% CS 410 Summer 2014 LaTeX template, based off of
% http://www.acm.org/sigs/publications/proceedings-templates

\documentclass{acm} % this finds the file "acm.bst"
\usepackage{easyfig}
\usepackage[utf8]{inputenc}
\usepackage{CJK,CJKspace,CJKpunct}
\usepackage{cite,graphicx}
% \usepackage[T1]{fontenc}

% this lets you include clickable URLs with \url{}
\usepackage[hidelinks]{hyperref}

\easyfigdefault{placement={!htb}, max width=20pc, max height=\textheight, keepaspectratio}

\pdfmapline{=unisong@Unicode@ <ipam.ttf}
\begin{document}\sloppy % sloppy necessary here
\begin{CJK}{UTF8}{zhsong}
\title{JLyrics
    \titlenote{Submitted as the final project for CS 410 Summer 2014.}
}

\numberofauthors{2} % make sure you set this number
\author{%
    \alignauthor Larry Resnik \\
    \affaddr{Department of Computer Engineering}\\
    \affaddr{University of Illinois at Urbana-Champaign}\\
    \email{lsresni2@illinois.edu}
    \alignauthor Zach Bian\\
    \affaddr{Department of Computer Engineering}\\
    \affaddr{University of Illinois at Urbana-Champaign}\\
    \email{zbian2@illinois.edu}
}
\maketitle

\begin{abstract}
JLyrics is a search engine that finds the Japanese song that some lyrics came from.
\end{abstract}

\keywords{japanese; music; search; anime; vocaloid}

\section{Introduction}

Our project was chosen to solve certain problems we had noticed in terms of information retrieval. For starters, looking up what song had a particular set of lyrics would not necessarily bring you to the same website storing those lyrics as another website also storing song lyrics. The information we want is sparsely distributed and uploaded. For example, if you do not know if the song originated from an anime or from a video game, you would not know whether to search an anime songs website or a video game songs website for the song. We wanted to combine the information sets of both or multiple websites so that you could be sure to run into the song you want to find.

\section{Motivation}

As for why someone would want to look up the song that some lyrics came from, there are a couple answers to that question.

First of all, a song one may come across by chance may have little information attached to it. Perhaps the song came from a medley or from a YouTube re-upload without a detailed title or tags. In that case, the listener has no background to go on to find the song aside from the lyrics they have heard. Moreover, human memory and recognition is finite. If we were to passively hear a song we like, we could only catch portions of the song's lyrics. Ideally, just those lyrics alone should be enough to help someone find that song.

Secondly, finding information on the song itself can assist the listener in learning foreign languages. Repetition is the best methods for learning. Listening to enjoyable music and recognizing new words each time the song is played is a fun way to strengthen one's language capabilities through repetition. Many music players such as foobar2000 and Rocket Player will display the lyrics of a song that is being played so long as those lyrics have been embedded into the music file as a tag. An example screenshot of Rocket Player showing a song's lyrics is in Figure~\ref{fig:res/rocket-player.png} This feature can be used in tandem with the learner's interest in following along with the song to promote learning the language. If one is clever, they could put the same lyrics into the karaoke variant of the song and practice singing to improve their reading and speaking abilities. Regardless of the reason, they user would need to find the full lyrics from the song. We want to help with this cause.

\Figure[caption="Rocket Player Displaying Song Lyrics]{res/rocket-player.png}

Finally, if someone finds a song they like, there is a good chance that the songs made by that song's producer would also be likable. 

\section{Goals}

We want to build a search engine that connects the world of Japanese songs to non-Japanese speakers. To that extent, we had a list of goals to work on to drive the development of the project.

Our target audience are people who are interested in songs sung in anime, in doujin music, or by Vocaloid artists. These interests tend to be intermixed because of websites that host all of these things at once such as Niconico Douga. Because the non-Japanese speaking fans of these materials are numerous, this search engine could be useful for them. The intermingling of sources creates a rich and diverse community, but the people who are introduced to the Japanese song scene would not necessarily understand the difference between a Vocaloid song and a Touhou doujin song for instance. Using our search engine should provide no boundaries to that inadequacy.

%Aside from that, the language barrier that a non-Japanese speaker has to deal with when looking up Japanese is having to figure out if their search results bore fruit or not. The first result may be a perfect match, but if the searcher can't read Japanese in the first place, they would have no idea just how accurate their search was. It would be nice if we could present to the user their search results in a format they could understand and work with such as translated text or transliterated text (written in romaji).

In order to merge these data points of interest into one search engine, we need to scour the Internet to get diverse sources of data. Although we want a lot of data, we also want to minimize overlap in the data we scour. Crawling the Internet is a costly operation in terms of time because we must wait between requests to get web pages. We decided to target three sites in particular in order to balance the amount of songs we could fetch and the estimated amount of overlap we may encounter.

\begin{itemize}
\item ピアプロ (Piapro): \url{http://piapro.jp/}
\item Anime Lyrics dot Com: \url{http://www.animelyrics.com/}
\item 東方同人ＣＤの歌詞 (Touhou Doujin CD no Kashi): \url{http://www31.atwiki.jp/touhoukashi/}
\end{itemize}

Piapro is a famous site for Vocaloid producers. They upload their songs, lyrics, karaoke versions of their songs, and other things to Piapro. It is a valuable site because a huge portion of songs uploaded to Niconico Douga in video format are also linked to Piapro. This typically is done to show fans to the karaoke versions of songs so that the fans could upload videos of themselves singing to the song. In other words, Piapro covers a huge fanbase for Vocaloid and 歌ってみた (utattemita / Tried to sing) fans.

Anime Lyrics dot Com would be our most valuable source of song data because, as the name implies, it is a massive database of anime songs and their lyrics. This site stores lyrics for the purpose of non-Japanese speakers, so it stores them transliterated first and foremost. The song may or may not have kanji and/or English lyrics available as well. Due to the uncertainty of which songs have lyrics or not and what formats are available, Anime Lyrics dot Com appears to be the site that will take the most development time to crawl.

Touhou Doujin CD no Kashi is a Wikipedia site dedicated to storing the lyrics of Touhou doujin songs. The Touhou fanbase has made a huge array of music and the fans are also numerous these days. On the same token, a listener may not recognize a song as a remix from a Touhou game, so this search engine would not be troubled by the discrepancy. Because Touhou Doujin CD no Kashi is a Wikipedia site, it would take a considerable amount of development time to scour and parse.


\section{Crawling}

Before we could make a search engine, we would need to have material to search through. The Internet already has a great amount of data on it, but we need to extract that information. In order to do that, we used programming languages that were able to retrieve web pages so that we could save those pages on our hard drives. Later, we would program some way to parse those pages and get the data we wanted. The final product would be a standardized format for documents (a.k.a. docs) that the indexer would interpret.

\subsection{Anime Lyrics dot Com}

Anime Lyrics dot Com is a multi-layered site, so it took a considerable amount of time determining how to best crawl it. The home page lists all of the genres of music stored on Anime Lyrics dot Com. Despite what their name may imply, they also store lyrics for dance, dance CD, doujin, video game, and J-Pop music. Each genre has an index.htm file listing all of the albums belonging to that category. Each album name points to an index.htm in a deeper directory. The album page at this area lists all of the songs in that album. The songs are in the same directory as the album, but they do not have the name index.htm, so they stand out in the address bar. The name index.htm is a default name that the web browser will try to access, so the URL usually omits this. By consequence, we only see a ".htm" URL when looking at a song.

Graphically, the general structure of Anime Lyrics dot Com can be seen in Figure~\ref{fig:res/animelyrics_format.png}. All folders shown have an index.htm in them, but the album folders all hold song web pages as well.

\Figure[caption={Anime Lyrics dot Com Structure}, max width=5pc]{res/animelyrics_format.png}

When the pages are parsed to generate the indexable docs, the folder structure is retained, but there will be no more index.htm pages left over.

All Anime Lyrics dot Com crawling and parsing was done in Python 2 using the Beautiful Soup (2) library and a few regular expressions. All crawling was done with a one-to-two second randomized delay so as to not hammer their server. Two scripts were ran at once simultaneously on the same computer to get more songs at once during the final phase of the crawling.

The genre pages were grabbed by hand and the full list of albums were retrieved from that. The album pages were stored locally, but it was quickly discovered that even the plain text produced by Anime Lyrics dot Com created an extremely large amount of data that was stored. About 60 MB of album pages were downloaded. Most of this was boiler plate code such as the navigation bar, embedded scripts, and formatting to make the web page display nicely.

Before beginning the crawling of the song pages, a method of getting rid of the unnecessary data was worked on. Without making too many assumptions about the format of the nearly 40,000 song pages Anime Lyrics dot Com had available, it was decided that the chunk of code above and below the song lists of each album page would be cropped out before being saved to the hard drive. As an extra precaution, Beautiful Soup was used to re-create valid HTML before saving the crawled album page. This was to ensure that later parsing of the song page would hopefully be consistent.

During the development process, a total of three different systems had seen work with the codebase being worked on. Python 2 with Beautiful Soup 2 had ran the script on Windows 7, Windows 8, and Xubuntu (Linux). In doing so, non-cross-platform bugs surfaced in the code. Of particular interest was that the crawler would outright crash on Windows. The problem was that Beautiful Soup was capable of using a slew of different HTML parsers and it would select a default for you. The default was usually a bad one that shipped with Python, so on the Windows operating systems, the lxml library was installed as well. With all three systems using lxml for parsing the web pages, the code became consistent in behavior and cross-platform.

12,654 songs were scoured.

\subsection{Piapro}

Piapro was a fairly simple site to crawl due to the site's structure. The pages were all indexed by number, which could be accessed via changing the URL itself. The first page could be accessed at \url{http://piapro.jp/text/?categoryId=7\&page=1}. Changing the URL page number past the last page returns a page that says ``見つかりませんでした。'' or ``Couldn't find it.'' Each of these 4710 pages lists thirty songs, for a grand total of just over 141,000 songs. These links are stored under \url{http://piapro.jp/t/} on the site itself. Each song has a unique id of four characters, such as ``KlF-'', which goes after the previous url. These pages contain the title, author, and lyrics data we needed.

As for implementation, at first C++ was used to call ``curl'' (via <cstdlib>'s ``system()'' function) first on the list pages to get all the song id's into a file. Next, the program went through this list of song id's and called curl again on each of the URL's, pulling the required data from the downloaded HTML file and writing them to files.

In order to parse each of these HTML files, since the characters were in UTF-8, a setlocale was required. Also, the glibmm library provides wrappers for processing UTF-8 strings with the Glib::ustring data type, which can be used much in the same way as the std::string data type. It also provides a regex library for UTF-8 regular expressions. As the pages were downloaded, each line was checked for mathcing regular expressions. The following regular expresions were used:

\begin{enumerate}
 \item (?<=/t/).\{4\} : Used to match song links because all songs had a four character id under the /t/ directory.
 \item (?<=「).*(?=」) : Used to match the title on the song page, since they were contained in the square open quote.
 \item (?<=$''$/).*?(?=$''$ class=$''$i\_icon) : Used to match the user name, since it always followed the tag with i\_icon in it.
 \item p id=$''$\_txt\_main$''$ : Used to find the first line of lyrics, which had a different pattern in the html document.
 \item (?<=>).*(?=<br) : Used to match the first line of lyrics
 \item \^{}.+(?=<br) : Used to match all other lyrics, each of which had a line break tag after it.
 \item \^{}.*(?=</p) : Used to match the last line of lyrics, which breaks out of the processing loop.
\end{enumerate}

Just for convenience, a progress checker that kept track of the number of songs was introduced. Furthermore, a time delay was used to prevent too many requests being sent at once. This was a small random integer of seconds.

Due to the sheer number of songs to crawl, it became necessary to distribute the processing over a number of computers. Another issue was that EWS lacks the required libraries to compile this code or even run the executable. As a result, a simplification needed to be made. This time, the crawler simply took the html files, so that they could all be processed on a machine with the required library files. In short, the original crawler was split into two parts. As a result, the following scheme was used: (Note: Ideally, the number of links would have been 141,303, but due to the site being updated, there were duplicate links.)

\begin{itemize}
 \item Links 1-69,999: Zach's laptop
 \item Links 70,000-79,999: Zach's laptop via ssh to EWS
 \item Links 80,000-99,999: EWS machine no. 1
 \item Links 100,000-141,357: EWS machine no. 2
 \item Total curl commands required: 146,067
\end{itemize}

Assuming an up-to-date system as of July 31, 2014, the following libraries were used in the first crawler: (Note: the split up crawler uses a combination of these.)

\begin{enumerate}
 \item glibmm/ustring.h : ustring data type and functions
 \item glibmm/regex.h : UTF-8 regex library
 \item libxml++ : required to use the above two
 \item iostream : streams
 \item cstdio : popen function (used to count lines in the linkFile)
 \item clocale : Set to UTF-8 mode
 \item fstream : fileio
 \item string : std::string for line-by-line reading
 \item sstream : string concatenation
 \item cstdlib : system() function, random() function
 \item ctime : seeds random()
 \item thread : sleep\_for() function
 \item chrono : provides sleep\_for with number of seconds.
\end{enumerate}

Compilation commands (Makefile):

\begin{enumerate}
 \item Zach's laptop: g++ -g -std=c++0x main.cpp -o main `pkg-config libxml++-2.6 --cflags glib-2.0 --libs glib-2.0`
 \item EWS machines (crawl): g++ -g -std=gnu++0x main.cpp -o main -D\_GLIBCXX\_USE\_NANOSLEEP
 \item Zach's laptop (after EWS crawl): g++ -g -std=c++0x postParse.cpp -o postParse `pkg-config libxml++-2.6 --cflags glib-2.0 --libs glib-2.0`
\end{enumerate}

The actual time spent crawling across the four machines was around 3 days. Otherwise, on a single machine, a reasonable estimate would have been around 5-6 days. Had we performed this estimation sooner, we could have distributed the download and saved another day or so.

\section{Results}

Due to time constraints imposed by other classes that both team members had to deal with, only the bare minimum working product with rudimentary testing in the end. The search engine works from command line because there was not enough time left over to try to convert our project into a website. Out of all of the websites we wanted to crawl, only Piapro and the Japanese text uploaded to Anime Lyrics dot Com were saved. So altogether, we crawled around 13,000+40,000+141,000+5000 = 199,000 pages.

Unfortunately, because the pre-processed HTML files are considerably larger than the post-processed ones, the EWS machines ran out of space. Due to this unforseen occurrence, we didn't think to save space by clearing out unnecessary files, and only around 1GB's worth of HTML files could be downloaded. In reality, the number of documents indexed only ended up being 127,223 instead of an estimated 154,000.

\subsection{Platform Dependent Results}

Unfortunately, the project would not work under Windows. Windows support was a feature that we assumed that we would get for free just because we used the cross-platform programming language Java to do our indexing and searching. Naively attempting to run the searcher in Windows will most likely evoke mojibake. 文字化け (mojibake meaning "changed characters") is the result of an encoding scheme being unable to properly represent a bit pattern.

In order to understand the problem with mojibake, a discussion of character encodings will need to be presented. A single character such as the letter "a" must be represented in a format that a computer can understand. At its absolute lowest level, a computer can only store data in the form of ones and zeros. The number system that contains only these two numbers is called binary. However, the string of bits we choose to encode a particular character in is completely arbitrary. To that extent, the ASCII (American Standard Code for Information Interchange) format was invented so that everyone programming on computers would follow the same format for encoding a set of characters. For example, "a" is represented in ASCII format as "0110 0001". ASCII was invented in the very young days of computing, so there was no intention of supporting alphabets and character sets beyond what Latin characters could represent (for example, English).

Meanwhile, Japan was making headway in computing but was in dire need of a standard way to represent Japanese characters. The format called Shift-JIS (Shift Japanese Industrial Standards) was invented. It was created for representing Japanese characters such as kanji, hiragana, and katakana in standardized bit patterns, but it was also capable of representing Latin characters (called romaji / ローマ字 / roman characters in Japanese). This format solved Japan's rendering problems, but it was not a very cooperative encoding format when interpreted by programs and operating systems not expecting the Shift-JIS format.

As time went on, a new standard of expressing characters as strings of bytes appeared called Unicode. It proposed blueprints of how one should represent characters form any and all alphabets in a set length of bits. Unicode is an abstract concept. The most popular encoding of Unicode to this day is UTF-8 (Unicode Transformation Format 8). Many Japanese developers are content with Shift-JIS even though the Shift-JIS encoding can't handle non-Japanese and non-Latin characters, so there is a struggle involved with dealing with Japanese text such that one must check if the text was encoded in Shift-JIS, UTF-8, or something else.

To illustrate just how drastically important keeping our encoding scheme straight is, a demonstration of various encodings of the same Japanese word is shown using Python 3.

\begin{verbatim}
>>> "文字"
'文字'
>>> "文字".encode("utf-8")
b'\xe6\x96\x87\xe5\xad\x97'
>>> "文字".encode("shift-jis")
b'\x95\xb6\x8e\x9a'
>>> "文字".encode("cp932")
b'\x95\xb6\x8e\x9a'
\end{verbatim}

This word, 文字 (moji meaning character(s)), shows its binary representations as it would be if it were encoded in UTF-8, Shift-JIS, and Windows Code Page 932. Notice how Windows CP-932 is the same as Shift-JIS.

The pages we took from Piapro were stored in UTF-8, but we had a good number of pages from Anime Lyrics dot Com that were stored in Shift-JIS. Parsing these web pages and producing indexable documents from them required careful attention of the encodings each web page was chosen to be encoded in.

Despite all efforts to keep our encodings straight, nothing could get Windows' command prompt, cmd, to behave as we would have wanted it to. As shown in Figure~\ref{fig:res/cmd_search_fail.png}, trying to look for the words 聞こえる and 愛 fail to produce results. We wonder if it is because the words we pass to the Java program through cmd are getting re-encoded and thereby messing up our results.

\Figure[caption="cmd.exe on Windows Failure"]{res/cmd_search_fail.png}

While testing our project, we had access to Linux computers through SSH thanks to the University of Illinois at Urbana Champaign's Engineering WorkStations (UIUC's EWS). By using the Windows program called PuTTY to interact with the Java-based searching program, we got to see perfect results as shown in Figure~\ref{fig:res/putty_search_success.png}. It raises the question of where the encoding transformations could be going wrong since PuTTY appears to be a terminal application just like cmd.

\Figure[caption="PuTTY on Windows Success"]{res/putty_search_success.png}

If we could output the search results to a web page, the encodings would be the problem of the web browser instead of the terminal. Because popular web browsers are intended to be used by people worldwide, they tend to support many text encoding formats, so there is may be better results by having a web browser interact with the Java program. We did not have the time to work on the web browser implementation though.

\subsection{Kana Searching}

\subsection{Google Custom Search}

\section{Related Work}

What similar things are already out there? You can cite things like
this~\cite{manning-ir-book} and this~\cite{zhai-smoothing}. I got the second
reference bibtex from \url{http://dl.acm.org/citation.cfm?id=984322}. I just
simplified the bibtex reference names to ``manning-ir-book'' and
``zhai-smoothing''. Also, did you notice that URL and citations were clickable?

\section{Experiments}

Did you perform any experiments?

Word distance

Comparison with Google Custom Search / Selective removal of text

Searching with many/few kanji

\section{Discussion}

Are there any interesting questions or findings that should be discussed?

\section{Conclusion}

Here's why our project is great, and this is what we told you in the paper.

\section{Future Plans}

Due to time constraints, we were unfortunately unable to implement some of the items that we wanted to get to.

Firstly, we think that the creation of a webpage would have been a nice addition to the project. To give a more concrete idea, certain websites have a built-in custom search bar and display search results in the browser. This would be a big advantage over the command line searcher because command line applications requires special knowledge of the command line and terminal applications can't handle Japanese characters if the font does not support the proper size of the characters (backspacing disaligns the cursor).

Another big issue we ran into was when we tried to run the application on Windows' command line. Windows has its own method of text-encoding. The first major consequence is that results do not render properly. For some reason, only some of the lyrics' kanji would show up correctly, but not all of them. The second major consequence is that the search does not perform properly. Searching for any Japanese text yields no results at all, and putting the same English query into Linux and Windows yield different results.

As for additional Japanese support, we could provide disambiguation of synonyms in the query. By synonym here, we mean that a word could have a kanji version(s), a kana version, and a romaji version, either in full- or half-width. For example, there are many ways to write the verb ``to take'' such as とる, 取る, and 撮る. These are all pronounced ``toru,'' but each has different nuances. We might not know which of these is in the song entry, so this would make searching easier on the user.

Speaking of synonyms and the user, the user may lack adequate knowledge of the Japanese language to read the search results. It would also be convenient to give the user the option of getting results in Japanese or its Romanized form.

We also wanted to crawl 東方同人ＣＤの歌詞 (Touhou Doujin CD no Kashi): \url{http://www31.atwiki.jp/touhoukashi/}. This is a giant database of songs that neither Anime Lyrics dot Com nor Piapro have. The Touhou fanbase has made many songs, so it would have given us a lot more data to work with. The creation of another crawler would have been a lot more work on its own, though.

Lastly, the addition of Anime Lyrics dot Com's transliterated lyrics and translated lyrics would have made a good addition to our database. Currently, our data only consists of Japanese songs, which requires some degree of knowledge of the language to use. With this, English speakers could search a English translation of the song as well.

\section*{Acknowledgments}

The authors would like to thank people for things. You can delete this section
if you want.

\appendix

\section{Division of Labor}

\begin{enumerate}
\item Crawling and Parsing of Anime Lyrics dot Com: Larry
\item Crawling and Parsing of Piapro: Zach
\item Initial Outline: Larry, Zach
\item Midterm Report: Larry, Zach
\item Lucene Setup: Larry
\end{enumerate}

\bibliographystyle{plain}
\bibliography{bib} % "bib" is the name of the .bib file

\end{CJK}
\end{document}
