% CS 410 Summer 2014 LaTeX template, based off of
% http://www.acm.org/sigs/publications/proceedings-templates

\documentclass{acm} % this finds the file "acm.bst"
\usepackage[utf8]{inputenc}
\usepackage{CJK,CJKspace,CJKpunct}
\usepackage{cite,graphicx}

% this lets you include clickable URLs with \url{}
\usepackage[hidelinks]{hyperref}

\pdfmapline{=unisong@Unicode@ <ipam.ttf}
\begin{document}\sloppy % sloppy necessary here
\begin{CJK}{UTF8}{zhsong}
\title{JLyrics
    \titlenote{Submitted as the final project for CS 410 Summer 2014.}
}

\numberofauthors{2} % make sure you set this number
\author{%
    \alignauthor Larry Resnik \\
    \affaddr{Department of Computer Engineering}\\
    \affaddr{University of Illinois at Urbana-Champaign}\\
    \email{lsresni2@illinois.edu}
    \alignauthor Zach Bian\\
    \affaddr{Department of Computer Engineering}\\
    \affaddr{University of Illinois at Urbana-Champaign}\\
    \email{zbian2@illinois.edu}
}
\maketitle

\begin{abstract}
JLyrics is a search engine that finds the Japanese song that some lyrics came from.
\end{abstract}

\keywords{japanese; music; search; anime; vocaloid}

\section{Introduction}

Our project was chosen to solve certain problems we had noticed in terms of information retrieval. For starters, looking up what song had a particular set of lyrics would not necessarily bring you to the same website storing those lyrics as another website also storing song lyrics. The information we want is sparsely distributed and uploaded. For example, if you do not know if the song originated from an anime or from a video game, you would not know whether to search an anime songs website or a video game songs website for the song. We wanted to combine the information sets of both or multiple websites so that you could be sure to run into the song you want to find.

\section{Motivation}

As for why someone would want to look up the song that some lyrics came from, there are a couple answers to that question.

First of all, a song one may come across by chance may have little information attached to it. Perhaps the song came from a medley or from a YouTube re-upload without a detailed title or tags. In that case, the listener has no background to go on to find the song aside from the lyrics they have heard. Moreover, human memory and recognition is finite. If we were to passively hear a song we like, we could only catch portions of the song's lyrics. Ideally, just those lyrics alone should be enough to help someone find that song.

Secondly, finding information on the song itself can assist the listener in learning foreign languages. Repetition is the best methods for learning. Listening to enjoyable music and recognizing new words each time the song is played is a fun way to strengthen one's language capabilities through repetition. Many music players such as foobar2000 and Rocket Player will display the lyrics of a song that is being played so long as those lyrics have been embedded into the music file as a tag. This feature can be used in tandem with the learner's interest in following along with the song to promote learning the language. If one is clever, they could put the same lyrics into the karaoke variant of the song and practice singing to improve their reading and speaking abilities. Regardless of the reason, they user would need to find the full lyrics from the song. We want to help with this cause.

TODO: Picture of Rocket Player or foobar2000

Finally, if someone finds a song they like, there is a good chance that the songs made by that song's producer would also be likable. 

\section{Goals}

We want to build a search engine that connects the world of Japanese songs to non-Japanese speakers. To that extent, we had a list of goals to work on to drive the development of the project.

Our target audience are people who are interested in songs sung in anime, in doujin music, or by Vocaloid artists. These interests tend to be intermixed because of websites that host all of these things at once such as Niconico Douga. Because the non-Japanese speaking fans of these materials are numerous, this search engine could be useful for them. The intermingling of sources creates a rich and diverse community, but the people who are introduced to the Japanese song scene would not necessarily understand the difference between a Vocaloid song and a Touhou doujin song for instance. Using our search engine should provide no boundaries to that inadequacy.

%Aside from that, the language barrier that a non-Japanese speaker has to deal with when looking up Japanese is having to figure out if their search results bore fruit or not. The first result may be a perfect match, but if the searcher can't read Japanese in the first place, they would have no idea just how accurate their search was. It would be nice if we could present to the user their search results in a format they could understand and work with such as translated text or transliterated text (written in romaji).

In order to merge these data points of interest into one search engine, we need to scour the Internet to get diverse sources of data. Although we want a lot of data, we also want to minimize overlap in the data we scour. Crawling the Internet is a costly operation in terms of time because we must wait between requests to get web pages. We decided to target three sites in particular in order to balance the amount of songs we could fetch and the estimated amount of overlap we may encounter.

\begin{itemize}
\item ピアプロ (Piapro): \url{http://piapro.jp/}
\item Anime Lyrics dot Com: \url{http://www.animelyrics.com/}
\item 東方同人ＣＤの歌詞 (Touhou Doujin CD no Kashi): \url{http://www31.atwiki.jp/touhoukashi/}
\end{itemize}

Piapro is a famous site for Vocaloid producers. They upload their songs, lyrics, karaoke versions of their songs, and other things to Piapro. It is a valuable site because a huge portion of songs uploaded to Niconico Douga in video format are also linked to Piapro. This typically is done to show fans to the karaoke versions of songs so that the fans could upload videos of themselves singing to the song. In other words, Piapro covers a huge fanbase for Vocaloid and 歌ってみた (utattemita / Tried to sing) fans.

Anime Lyrics dot Com would be our most valuable source of song data because, as the name implies, it is a massive database of anime songs and their lyrics. This site stores lyrics for the purpose of non-Japanese speakers, so it stores them transliterated first and foremost. The song may or may not have kanji and/or English lyrics available as well. Due to the uncertainty of which songs have lyrics or not and what formats are available, Anime Lyrics dot Com appears to be the site that will take the most development time to crawl.

Touhou Doujin CD no Kashi is a Wikipedia site dedicated to storing the lyrics of Touhou doujin songs. The Touhou fanbase has made a huge array of music and the fans are also numerous these days. On the same token, a listener may not recognize a song as a remix from a Touhou game, so this search engine would not be troubled by the discrepancy. Because Touhou Doujin CD no Kashi is a Wikipedia site, it would take a considerable amount of development time to scour and parse.

\section{Results}

Due to time constraints imposed by other classes that both team members had to deal with, only the bare minimum working product with rudimentary testing in the end. The search engine works from command line because there was not enough time left over to try to convert our project into a website. Out of all of the websites we wanted to crawl, only Piapro and the Japanese text uploaded to Anime Lyrics dot Com were saved. Altogether, we crawled around 60,000 pages.

\subsection{Platform Dependent Results}

Unfortunately, the project would not work under Windows. Windows support was a feature that we assumed that we would get for free just because we used the cross-platform programming language Java to do our indexing and searching. Naively attempting to run the searcher in Windows will most likely evoke mojibake. 文字化け (mojibake meaning "changed characters") is the result of an encoding scheme being unable to properly represent a bit pattern.

In order to understand the problem with mojibake, a discussion of character encodings will need to be presented. A single character such as the letter "a" must be represented in a format that a computer can understand. At its absolute lowest level, a computer can only store data in the form of ones and zeros. The number system that contains only these two numbers is called binary. However, the string of bits we choose to encode a particular character in is completely arbitrary. To that extent, the ASCII (American Standard Code for Information Interchange) format was invented so that everyone programming on computers would follow the same format for encoding a set of characters. For example, "a" is represented in ASCII format as "0110 0001". ASCII was invented in the very young days of computing, so there was no intention of supporting alphabets and character sets beyond what Latin characters could represent (for example, English).

Meanwhile, Japan was making headway in computing but was in dire need of a standard way to represent Japanese characters. The format called Shift-JIS (Shift Japanese Industrial Standards) was invented. It was created for representing Japanese characters such as kanji, hiragana, and katakana in standardized bit patterns, but it was also capable of representing Latin characters (called romaji / ローマ字 / roman characters in Japanese). This format solved Japan's rendering problems, but it was not a very cooperative encoding format when interpreted by programs and operating systems not expecting the Shift-JIS format.

As time went on, a new standard of expressing characters as strings of bytes appeared called Unicode. It proposed blueprints of how one should represent characters form any and all alphabets in a set length of bits. Unicode is an abstract concept. The most popular encoding of Unicode to this day is UTF-8 (Unicode Transformation Format 8). Many Japanese developers are content with Shift-JIS even though the Shift-JIS encoding can't handle non-Japanese and non-Latin characters, so there is a struggle involved with dealing with Japanese text such that one must check if the text was encoded in Shift-JIS, UTF-8, or something else.

To illustrate just how drastically important keeping our encoding scheme straight is, a demonstration of various encodings of the same Japanese word is shown using Python 3.

\begin{verbatim}
>>> "文字"
'文字'
>>> "文字".encode("utf-8")
b'\xe6\x96\x87\xe5\xad\x97'
>>> "文字".encode("shift-jis")
b'\x95\xb6\x8e\x9a'
>>> "文字".encode("cp932")
b'\x95\xb6\x8e\x9a'
\end{verbatim}

This word, 文字 (moji meaning character(s)), shows its binary representations as it would be if it were encoded in UTF-8, Shift-JIS, and Windows Code Page 932. Notice how Windows CP-932 is the same as Shift-JIS.

The pages we took from Piapro were stored in UTF-8, but we had a good number of pages from Anime Lyrics dot Com that were stored in Shift-JIS. Parsing these web pages and producing indexable documents from them required careful attention of the encodings each web page was chosen to be encoded in.

Despite all efforts to keep our encodings straight, nothing could get Windows' command prompt, cmd, to behave as we would have wanted it to.

TODO: Add pictures of command prompts

\subsection{Kana Searching}

\subsection{Google Custom Search}

\section{Related Work}

What similar things are already out there? You can cite things like
this~\cite{manning-ir-book} and this~\cite{zhai-smoothing}. I got the second
reference bibtex from \url{http://dl.acm.org/citation.cfm?id=984322}. I just
simplified the bibtex reference names to ``manning-ir-book'' and
``zhai-smoothing''. Also, did you notice that URL and citations were clickable?

\section{Crawling}

\subsection{Piapro}

Piapro was a fairly simple site to crawl due to the site's structure. The pages were all indexed by number, which could be accessed via changing the URL itself. The first page could be accessed by this URL: ``http://piapro.jp/text/?categoryId=7\&page=1''. Changing the URL page number past the last page returns a page that says ``見つかりませんでした。'' or ``Couldn't find it.'' Each of these pages lists ten songs, for a grand total of just over 47,000 songs. These links are stored under ``http://piapro.jp/t/'' on the site itself. Each song has a unique id of four characters, such as ``KlF-'', which goes after the previous url. These pages contain the title, author, and lyrics data we needed.

As for implementation, at first C++ was used to call ``curl'' (via <cstdlib>'s ``system()'' function) first on the list pages to get all the song id's into a file. Next, the program went through this list of song id's and called curl again on each of the URL's, pulling the required data from the downloaded HTML file and writing them to files.

In order to parse each of these HTML files, since the characters were in UTF-8, a setlocale was required. Also, the glibmm library provides wrappers for processing UTF-8 strings with the Glib::ustring data type, which can be used much in the same way as the std::string data type. It also provides a regex library for UTF-8 regular expressions. As the pages were downloaded, each line was checked for mathcing regular expressions. The following regular expresions were used:

\begin{enumerate}
 \item (?<=/t/).\{4\} : Used to match song links because all songs had a four character id under the /t/ directory.
 \item (?<=「).*(?=」) : Used to match the title on the song page, since they were contained in the square open quote.
 \item (?<=\"/).*?(?=\" class=\"i\_icon) : Used to match the user name, since it always followed the tag with i\_icon in it.
 \item p id=\"\_txt\_main\" : Used to find the first line of lyrics, which had a different pattern in the html document.
 \item (?<=>).*(?=<br) : Used to match the first line of lyrics
 \item \^.+(?=<br) : Used to match all other lyrics, each of which had a line break tag after it.
 \item \^.*(?=</p) : Used to match the last line of lyrics, which breaks out of the processing loop.
\end{enumerate}

Just for convenience, a progress checker that kept track of the number of songs was introduced. Furthermore, a time delay was used to prevent too many requests being sent at once. This was a small random integer of seconds.

Due to the sheer number of songs to crawl, it became necessary to distribute the processing over a number of computers. Another issue was that EWS lacks the required libraries to compile this code or even run the executable. As a result, a simplification needed to be made. This time, the crawler simply took the html files, so that they could all be processed on a machine with the required library files. In short, the original crawler was split into two parts. As a result, the following scheme was used:

\begin{enumerate}
 \item Pages 1-69,999: Zach's laptop
 \item Pages 70,000-79,999: Zach's laptop via ssh to EWS
 \item Pages 80,000-99,999: EWS machine no. 1
 \item Pages 100,000-141,357: EWS machine no. 2
\end{enumerate}

Assuming an up-to-date system as of July 31, 2014, the following libraries were used in the first crawler: (Note: the split up crawler uses a combination of these.)

\begin{enumerate}
 \item glibmm/ustring.h : ustring data type and functions
 \item glibmm/regex.h : UTF-8 regex library
 \item libxml++ : required to use the above two
 \item iostream : streams
 \item cstdio : popen function (used to count lines in the linkFile)
 \item clocale : Set to UTF-8 mode
 \item fstream : fileio
 \item string : std::string for line-by-line reading
 \item sstream : string concatenation
 \item cstdlib : system() function, random() function
 \item ctime : seeds random()
 \item thread : sleep\_for() function
 \item chrono : provides sleep\_for with number of seconds.
\end{enumerate}

Compilation commands (Makefile):

\begin{enumerate}
 \item Zach's laptop: g++ -g -std=c++0x main.cpp -o main `pkg-config libxml++-2.6 --cflags glib-2.0 --libs glib-2.0`
 \item EWS machines (crawl): g++ -g -std=gnu++0x main.cpp -o main -D\_GLIBCXX\_USE\_NANOSLEEP
 \item Zach's laptop (after EWS crawl): g++ -g -std=c++0x postParse.cpp -o postParse `pkg-config libxml++-2.6 --cflags glib-2.0 --libs glib-2.0`
\end{enumerate}

\section{Experiments}

Did you perform any experiments?

\section{Discussion}

Are there any interesting questions or findings that should be discussed?

\section{Conclusion}

Here's why our project is great, and this is what we told you in the paper.

\section{Future Plans}

Due to time constraints, we were unfortunately unable to implement some of the items that we wanted to get to.

Firstly, we think that the creation of a webpage would have been a nice addition to the project. To give a more concrete idea, certain websites have a built-in custom search bar and display search results in the browser. This would be a big advantage over the command line searcher because command line applications requires special knowledge of the command line and terminal applications can't handle Japanese characters if the font does not support the proper size of the characters (backspacing disaligns the cursor).

Another big issue we ran into was when we tried to run the application on Windows' command line. Windows has its own method of text-encoding. The first major consequence is that results do not render properly. For some reason, only some of the lyrics' kanji would show up correctly, but not all of them. The second major consequence is that the search does not perform properly. Searching for any Japanese text yields no results at all, and putting the same English query into Linux and Windows yield different results.

As for additional Japanese support, we could provide disambiguation of synonyms in the query. By synonym here, we mean that a word could have a kanji version(s), a kana version, and a romaji version, either in full- or half-width. For example, there are many ways to write the verb ``to take'' such as とる, 取る, and 撮る. These are all pronounced ``toru,'' but each has different nuances. We might not know which of these is in the song entry, so this would make searching easier on the user.

Speaking of synonyms and the user, the user may lack adequate knowledge of the Japanese language to read the search results. It would also be convenient to give the user the option of getting results in Japanese or its Romanized form.

We also wanted to crawl 東方同人ＣＤの歌詞 (Touhou Doujin CD no Kashi): \url{http://www31.atwiki.jp/touhoukashi/}. This is a giant database of songs that neither Anime Lyrics dot Com nor Piapro have. The Touhou fanbase has made many songs, so it would have given us a lot more data to work with. The creation of another crawler would have been a lot more work on its own, though.

Lastly, the addition of Anime Lyrics dot Com's transliterated lyrics and translated lyrics would have made a good addition to our database. Currently, our data only consists of Japanese songs, which requires some degree of knowledge of the language to use. With this, English speakers could search a English translation of the song as well.

\section*{Acknowledgments}

The authors would like to thank people for things. You can delete this section
if you want.

\appendix

\section{Division of Labor}

\begin{enumerate}
\item Crawling and Parsing of Anime Lyrics dot Com: Larry
\item Crawling and Parsing of Piapro: Zach
\item Initial Outline: Larry, Zach
\item Midterm Report: Larry, Zach
\item Lucene Setup: Larry
\end{enumerate}

\bibliographystyle{plain}
\bibliography{bib} % "bib" is the name of the .bib file

\end{CJK}
\end{document}
